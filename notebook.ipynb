{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7DZa4UUDNw2"
   },
   "source": [
    "# Wind Power forecasting for the day-ahead energy market - Data Challenge\n",
    "by Compagnie Nationale du Rhône, ENS Paris & Collège de France\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cap.img.pmdstatic.net/fit/http.3A.2F.2Fprd2-bone-image.2Es3-website-eu-west-1.2Eamazonaws.2Ecom.2Fcap.2F2019.2F10.2F04.2Fea495374-9115-4be7-a91a-e9bc5b305b0b.2Ejpeg/768x432/background-color/ffffff/focus-point/992%2C1086/quality/70/dangereuses-pour-la-sante-peu-ecolo-faut-il-en-finir-avec-les-eoliennes-1352031.jpg\" width=\"600\"/></p>\n",
    "\n",
    "Challenge website: https://challengedata.ens.fr/participants/challenges/34/\n",
    "\n",
    "The objective of this challenge is to **design and train an ML/DL model to predict the hourly electrical production** of six independent wind farms owned by CNR for the day ahead, using multiple Numerical Weather Predictions (NWP) models. This is a **supervised learning problem** based on **multivariate time series**.\n",
    "\n",
    "This notebook is **fully compatible with Google Colab**, feel free to try it yourself!\n",
    "https://colab.research.google.com/github/qcha41/wind-power-forecasting-challenge/blob/master/notebook.ipynb\n",
    "\n",
    "## Notebook setup\n",
    "\n",
    "First of all, let's import the required libraries and configure the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab configuration\n",
    "!git clone https://github.com/qcha41/wind-power-forecasting-challenge.git\n",
    "!pip install urllib3==1.25.4 folium==0.2.1 boto3 mlflow mpld3 --quiet\n",
    "%cd wind-power-forecasting-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22601,
     "status": "ok",
     "timestamp": 1612551946501,
     "user": {
      "displayName": "Quentin Chateiller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguKgZaCTK0rxnxTJIqcAU2_SCRshI2HzPX7stG7AY=s64",
      "userId": "11797464397335309536"
     },
     "user_tz": -60
    },
    "id": "hf8u3qX7DNw4"
   },
   "outputs": [],
   "source": [
    "# Load and configure libraires\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import core\n",
    "import mlflow, mlflow.tensorflow\n",
    "import mpld3\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from importlib import reload \n",
    "mlflow.tensorflow.autolog(every_n_iter=1,log_models=False)\n",
    "sns.plotting_context('notebook', rc={'xtick.labelsize': 12})\n",
    "#mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PRIVATE CELL - SKIP IT]\n",
    "# Custom Google Colab configuration\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "cred = pd.read_csv('/content/gdrive/MyDrive/wind_power_forecasting_challenge_aws_credentials.csv',index_col=0, squeeze=True)\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = cred.AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = cred.AWS_SECRET_ACCESS_KEY\n",
    "mlflow.set_tracking_uri(f\"http://{cred.AWS_USERNAME}:{cred.AWS_PASSWORD}@{cred.AWS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unrWlVffDNw5"
   },
   "source": [
    "## Data\n",
    "### First exploration\n",
    "\n",
    "In this challenge, we are provided with a **training dataset** and a **test dataset**.\n",
    "\n",
    "The **training dataset** is composed of different hourly weather forecasts (X) for a period of 8 consecutive months (from May the 1st of 2018 to January the 15th of 2019), together with the associated observed power production in MW (Y). In the **test dataset**, only predictions are provided for another period of 8 months (January the 16th of 2019 to September the 30rd of 2019). The performance of our model is then evaluated online, by submitting its predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZ-PrOMhDNw5"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_raw = core.load_data()\n",
    "df_raw.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A given **training example** is thus composed of :\n",
    " - a **target time** (*Time* column) and a **wind farm ID** (*WF* column).\n",
    " - several **weather forecasts** for that (*Time*,*WF*) couple, in the form of *runs* (*NWP\\<i>_\\<HourOfTheRun>_\\<DayOfTheRun>_\\<Variable>* columns). Each run provides an estimation of a particular weather *Variable*, produced at a given time before the target *Time* (*HourOfTheRun*, *DayOfTheRun*), and coming from a given NWP models (*i*). For instance, the run *NWP1_00h_D-2_U* is estimating the weather variable *U* for a given target *Time* using the first NWP model, and is produced at midnight two days before the target *Time*. \n",
    " - the **observed power production** (*Production* column) for that (*Time*,*WF*) couple.\n",
    "\n",
    "The runs are coming from 4 different NWP models ($i\\in[1,4]$), and are forecasting 4 weather variables at various time:\n",
    " \n",
    "NWP Variable | Prediction description | NWP 1 (hourly) | NWP 2 (every 3 hours) | NWP 3 (every 3 hours) | NWP 4 (hourly)\n",
    "------ | ----- | ----- | ----- | ----- | -----\n",
    "Wind speed U,V (m/s) | 10min average [H-10min,H] | x (@100m) | x (@100m) | x (@100m) | x (@10m)\n",
    "Temperature of air T (m/s) | 1hour average [H-1,H] | x |  | x |\n",
    "Total cloud cover CLCT (%) | instant value at H | | | | x\n",
    "\n",
    "Further details about these forecasts can be found on the challenge webpage (link above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of training examples per wind farm\n",
    "df_raw.assign(training = df_raw.Production.isna(), test = ~df_raw.Production.isna())[['WF','training','test']].groupby('WF').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reshaping\n",
    "\n",
    "In order to train a model, we first need to design and shape the training examples that will feed it. In this problem, the learning features are the different weather forecasts, and the target output is the observed power production. \n",
    "\n",
    "Here are some characteristics of the NWP forecasts:\n",
    " - some NWP models are not forecasting their weather variables hourly.\n",
    " - a NWP model is forecasting a given weather variable several times before the target time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some NWP models are not forecasting a given weather variable hourly\n",
    "df_raw.loc[51:54,['WF','Time']+[f'NWP{i}_00h_D-2_U' for i in range(1,5)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given target time, a NWP model is forecasting a weather variable several times (at different delays before the target time).\n",
    "df_raw.loc[51:54,['WF','Time']+[col for col in df_raw.columns if col.startswith('NWP4') and col.endswith('_U')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to deal with the induced missing values is to **compute a new matrix containing the best weather forecast for each (*WF*, *NWP*, *Variable*) triplet**. In other words, each of these triplet is reduced to only one value. For instance, I will create a new single feature called *NWP4_U* giving the best forecast for the wind component *U* forecasted by the fourth NWP model, using the different forecasts *NWP4_XXh-XXX_U*.\n",
    "\n",
    "To do that, I am using a **weighted mean of the forecasts with a memory coefficient <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha\"/>** as hyperparameter. This allows to make recent forecasts more predominant in the calculation than the older ones. We have then : \n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V_{best}=\\dfrac{\\sum_{k=1}^{n}\\alpha^{\\Delta H_k}\\,V_k}{\\sum_{k=1}^{n}\\alpha^{\\Delta H_k}}\"/>\n",
    "\n",
    "where <img src=\"https://render.githubusercontent.com/render/math?math=V_k\"/> is the k-th prediction made for a given triplet, which has been produced <img src=\"https://render.githubusercontent.com/render/math?math=\\Delta H_k\"/> hours before the target time. \n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=\\alpha\"/> is a memory coefficient lying in <img src=\"https://render.githubusercontent.com/render/math?math=]0,1]\"/>, which make the value weight <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha^{\\Delta H_k}\"/> decaying as the delay <img src=\"https://render.githubusercontent.com/render/math?math=\\Delta H_k\"/> increases. If <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha=1\"/>, all predictions have the same weight (classic mean). Instead, if <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha\"/> tends towards 0, we are just keeping the more recent forecast. \n",
    "\n",
    "Let's start with <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha=0.9\"/>, meaning that the (H-1) forecast, if existing, has weight 0.9, then the (H-2) forecast has weight <img src=\"https://render.githubusercontent.com/render/math?math=(0.9)^2=0.81\"/>, the (H-12) forecast has weight <img src=\"https://render.githubusercontent.com/render/math?math=(0.9)^12=0.28\"/>, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute best weather forecasts\n",
    "FORECAST_MEMORY = 0.9\n",
    "df_best = core.calculate_best_forecasts(df_raw, FORECAST_MEMORY)\n",
    "df_best.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Previously, we have drastically reduced the number of original features by making new ones, more meaningful for training the future ML/DL model. But we are still facing missing values, due to the fact that there are no forecasts at all for these weather variable at these times. A fairly straightforward approach here is thus to **linearly interpolate these missing values using the forecasts made for previous and future times**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate remaining missing values\n",
    "df_interp = core.interpolate_nans(df_best)\n",
    "df_interp.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help the future ML/DL model to understand better the important features of the dataset, we create some of them based on the existing ones:\n",
    "- Wind speed <img src=\"https://render.githubusercontent.com/render/math?math=WS=U^2%2BV^2\"/>\n",
    "- Wind direction <img src=\"https://render.githubusercontent.com/render/math?math=WD=\\arctan(U/V)\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_aug = core.augment_data(df_interp)\n",
    "df_aug.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data insights\n",
    "\n",
    "Let's have a quick look on basic dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final overview\n",
    "df_aug.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore a bit more how the features evolves in time. Note the seasonality in the temperature feature. Unfortunately, this seasonality cannot be removed from the dataset because de training period is too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.plot_feature_vs_time(df_aug,\"T\") # Try also with T, U, V, CLCT, WS, WD variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore how the power production (target output) evolves in time for each wind farms. We can see that for instance that the maximal power production authorized is reached in WF1 and WF6, and that this limit is different from a wind farm to another (limited at 10 MW for WF1 and at 4 MW for WF6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.plot_production_vs_time(df_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following correlation graph confirms the high importance of having computed the wind speed *WS*: it is the feature that correlate the most with the power production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.plot_correlation_graph(core.mean_data(df_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see on the following graph (Production vs WS) that:\n",
    " - there is, on average, a linear relationship between the wind speed and power production.\n",
    " - the maximal power production capacity is limited differently regarding the wind farms. \n",
    " - WF1 seems to contain a period of forced inactivity (high WS forecasted but 0 MW of power production), which should be excluded from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.plot_feature_vs_production(df_aug,'WS') # Can be tried also with T, U, V, CLCT, WS, WD variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "- Standard normalization (z-score) for weather variables *U*, *V*, *T*, *WS*, *WD*.\n",
    "- Rescaling weather variable *CLCT* percentage between 0 and 1.\n",
    "\n",
    "The power production is not rescaled (values lying in [0,14])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqDy19JNDNw6"
   },
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "df = core.normalize_data(df_aug)\n",
    "df.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piGHgpwSDNw6"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYtVmg6WdGjc"
   },
   "outputs": [],
   "source": [
    "# Model parameters\r\n",
    "WINDOW_SIZE = 72  # In hours\r\n",
    "BATCH_SIZE = 2000\r\n",
    "EPOCHS = 10\r\n",
    "UNITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF1wTcwTDNw6"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(t_train, x_train, y_train, t_valid, x_valid, y_valid):\n",
    "\n",
    "    mlflow.log_params({'window_size':WINDOW_SIZE, 'units':UNITS, 'layer_type':'GRU'})\n",
    "    \n",
    "    # Make learning datasets\n",
    "    dataset_train = core.get_windowed_dataset(x_train, y_train, WINDOW_SIZE, BATCH_SIZE, shuffle=True)\n",
    "    dataset_valid = core.get_windowed_dataset(x_valid, y_valid, WINDOW_SIZE, BATCH_SIZE, shuffle=False) if x_valid is not None else None\n",
    "        \n",
    "    # Define model\n",
    "    model = tf.keras.Sequential([\n",
    "                tf.keras.layers.InputLayer(input_shape=next(iter(dataset_train))[0].shape[1:]),\n",
    "                tf.keras.layers.GRU(UNITS, return_sequences=True),\n",
    "                tf.keras.layers.Dropout(0.6),\n",
    "                tf.keras.layers.GRU(UNITS, return_sequences=True),\n",
    "                tf.keras.layers.Dropout(0.6),\n",
    "                tf.keras.layers.GRU(UNITS),\n",
    "                tf.keras.layers.Dense(1, activation='relu')\n",
    "            ])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=tf.keras.optimizers.Adam())\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(dataset_train, \n",
    "                        validation_data=dataset_valid,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=1, \n",
    "                        callbacks=[])#tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001,patience=30)])\n",
    "    core.plot_learning_curves(history)\n",
    "    \n",
    "    # Check predictions\n",
    "    y_train_predict = core.predict(model, dataset_train, t_train)\n",
    "    core.plot_predictions(t_train, y_train, y_train_predict, 'train')\n",
    "    if dataset_valid is not None :\n",
    "        y_valid_predict = core.predict(model, dataset_valid, t_valid)\n",
    "        core.plot_predictions(t_valid, y_valid, y_valid_predict, 'valid')\n",
    "            \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l1P4URCZrUT"
   },
   "source": [
    "# Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN6vRa-rdZcl"
   },
   "outputs": [],
   "source": [
    "HOLDOUT_VAL_SPLIT = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08_Y_Us5DNw6"
   },
   "outputs": [],
   "source": [
    "# TRAIN ONLY ONE WIND FARM\n",
    "# ================================\n",
    "def train_holdout_validation(wf_num, nested_run=False) :\n",
    "\n",
    "    with mlflow.start_run(nested=nested_run):    \n",
    "        mlflow.log_params({'wf':wf_num, 'split':HOLDOUT_VAL_SPLIT})\n",
    "\n",
    "        # Extract wf data\n",
    "        df_wf = core.extract_wf_data(df, wf_num)\n",
    "        \n",
    "        # Train\n",
    "        t_train, x_train, y_train, t_valid, x_valid, y_valid = core.split_holdout_validation(df_wf, HOLDOUT_VAL_SPLIT, WINDOW_SIZE)\n",
    "        model, history = train_model(t_train, x_train, y_train, t_valid, x_valid, y_valid)\n",
    "            \n",
    "    return model, history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50QVcEqgzlzc"
   },
   "outputs": [],
   "source": [
    "# Train one wind farm\n",
    "mlflow.set_experiment('holdout_validation')\n",
    "model, history = train_holdout_validation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY_7A48AOMCF"
   },
   "outputs": [],
   "source": [
    "# Train all wind farms\n",
    "mlflow.set_experiment('holdout_validation')\n",
    "with mlflow.start_run() :\n",
    "    for wf_num in df.WF.unique(): \n",
    "        train_holdout_validation(wf_num, nested_run = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4xRxSDtv0e3"
   },
   "source": [
    "# Forward chaining validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoLW8rJ7d-Fg"
   },
   "outputs": [],
   "source": [
    "# Forward chaining parameters\r\n",
    "FC_VAL_NB = 4\r\n",
    "FC_VAL_SIZE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdriGb82v0e3"
   },
   "outputs": [],
   "source": [
    "def train_forward_chaining_validation(wf_num):\n",
    "\n",
    "    # Extract wf data\n",
    "    df_wf = core.extract_wf_data(df, wf_num)\n",
    "    \n",
    "    # Train models\n",
    "    metrics = []\n",
    "    datas = core.split_forward_chaining_validation(df_wf, FC_VAL_SIZE, FC_VAL_NB, WINDOW_SIZE)\n",
    "    for (t_train, x_train, y_train, t_valid, x_valid, y_valid) in datas :\n",
    "        with mlflow.start_run(nested=True) :\n",
    "            model, history = train_model(t_train, x_train, y_train, t_valid, x_valid, y_valid)\n",
    "            metrics.append(history.history)\n",
    "    \n",
    "    # Calculate mean and std errors\n",
    "    metrics = core.get_mean_std_metrics(metrics)\n",
    "    mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKdYEe9Ox1k_"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment('forward_chaining_validation')\n",
    "for wf_num in df.WF.unique():\n",
    "    with mlflow.start_run():    \n",
    "        mlflow.log_params({'wf':wf_num,'valid_size':FC_VAL_SIZE,'nb_valid':FC_VAL_NB, 'nlayers':3, \n",
    "                        'layer_type':'GRU','units':UNITS,'epochs':EPOCHS, 'parent':True})\n",
    "        train_forward_chaining_validation(wf_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC0ea6kQgr1J"
   },
   "outputs": [],
   "source": [
    "wf_num = 4\r\n",
    "mlflow.set_experiment('forward_chaining_validation')\r\n",
    "for UNITS in [32,64] :\r\n",
    "    with mlflow.start_run() :\r\n",
    "        mlflow.log_params({'wf':wf_num,'valid_size':FC_VAL_SIZE,'nb_valid':FC_VAL_NB, 'nlayers':1, \r\n",
    "                           'layer_type':'GRU', 'units':UNITS, 'epochs':EPOCHS, 'parent':True })\r\n",
    "        train_forward_chaining_validation(wf_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK3nZypssKI2"
   },
   "source": [
    "# Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6SuOmlPDNw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN ALL WIND FARMS AND PREDICT\n",
    "# ================================\n",
    "def train_full(wf_num, nested_run=False) :\n",
    "    with mlflow.start_run(nested=nested_run): \n",
    "        mlflow.log_param('wf',wf_num)\n",
    "\n",
    "        # Extract data\n",
    "        df_wf = core.extract_wf_data(df, wf_num)        \n",
    "        \n",
    "        # Train model\n",
    "        t_train, x_train, y_train = core.get_train_dataset(df_wf, WINDOW_SIZE)\n",
    "        model, history = train_model(t_train, x_train, y_train, None, None, None)\n",
    "\n",
    "        # Predict on test data\n",
    "        t_test, x_test = core.get_test_dataset(df_wf, WINDOW_SIZE)\n",
    "        dataset_test = core.get_windowed_dataset(x_test, None, WINDOW_SIZE, BATCH_SIZE, shuffle=False)\n",
    "        y_test_predict = core.predict(model, dataset_test, t_test)    \n",
    "        core.plot_predictions(t_test, None, y_test_predict, 'test')\n",
    "\n",
    "    return y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67BQ2u3Azlzh"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment('Full training')\n",
    "with mlflow.start_run():\n",
    "    predictions = [train_full(wf_num, nested_run=True) for wf_num in df.WF.unique()]\n",
    "    core.save_predictions(predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_testsetval.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wind-power-forecasting-challenge",
   "language": "python",
   "name": "wind-power-forecasting-challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
