{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7DZa4UUDNw2"
   },
   "source": [
    "# Wind Power forecasting for the day-ahead energy market - Data Challenge\n",
    "by Compagnie Nationale du Rhône, ENS Paris & Collège de France\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cap.img.pmdstatic.net/fit/http.3A.2F.2Fprd2-bone-image.2Es3-website-eu-west-1.2Eamazonaws.2Ecom.2Fcap.2F2019.2F10.2F04.2Fea495374-9115-4be7-a91a-e9bc5b305b0b.2Ejpeg/768x432/background-color/ffffff/focus-point/992%2C1086/quality/70/dangereuses-pour-la-sante-peu-ecolo-faut-il-en-finir-avec-les-eoliennes-1352031.jpg\" width=\"600\"/></p>\n",
    "\n",
    "Challenge website: https://challengedata.ens.fr/participants/challenges/34/\n",
    "\n",
    "The objective of this challenge is to **design and train an ML/DL model to predict the hourly electrical production** of six independent wind farms owned by CNR for the day ahead, using multiple Numerical Weather Predictions (NWP) models. This is a **supervised learning problem** based on **multivariate time series**.\n",
    "\n",
    "This notebook is **fully compatible with Google Colab**, feel free to try it yourself!\n",
    "https://colab.research.google.com/github/qcha41/wind-power-forecasting-challenge/blob/master/notebook.ipynb\n",
    "\n",
    "## Notebook setup\n",
    "\n",
    "First of all, let's import the required libraries and configure the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab configuration\n",
    "!git clone https://github.com/qcha41/wind-power-forecasting-challenge.git\n",
    "!pip install urllib3==1.25.4 folium==0.2.1 boto3 mlflow mpld3 --quiet\n",
    "%cd wind-power-forecasting-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 22601,
     "status": "ok",
     "timestamp": 1612551946501,
     "user": {
      "displayName": "Quentin Chateiller",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguKgZaCTK0rxnxTJIqcAU2_SCRshI2HzPX7stG7AY=s64",
      "userId": "11797464397335309536"
     },
     "user_tz": -60
    },
    "id": "hf8u3qX7DNw4"
   },
   "outputs": [],
   "source": [
    "# Load and configure libraires\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import core\n",
    "import mlflow, mlflow.tensorflow\n",
    "import mpld3\n",
    "mlflow.tensorflow.autolog(every_n_iter=1,log_models=False)\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PRIVATE CELL - SKIP IT]\n",
    "# Custom Google Colab configuration\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "cred = pd.read_csv('/content/gdrive/MyDrive/wind_power_forecasting_challenge_aws_credentials.csv',index_col=0, squeeze=True)\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = cred.AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = cred.AWS_SECRET_ACCESS_KEY\n",
    "mlflow.set_tracking_uri(f\"http://{cred.AWS_USERNAME}:{cred.AWS_PASSWORD}@{cred.AWS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unrWlVffDNw5"
   },
   "source": [
    "## Data\n",
    "### First exploration\n",
    "\n",
    "In this challenge, we are provided with a **training dataset** and a **test dataset**.\n",
    "\n",
    "The **training dataset** is composed of different hourly weather forecasts (X) for a period of 8 consecutive months (from May the 1st of 2018 to January the 15th of 2019), together with the associated observed power production in MW (Y). In the **test dataset**, only predictions are provided for another period of 8 months (January the 16th of 2019 to September the 30rd of 2019). The performance of our model is then evaluated online, by submitting its predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TZ-PrOMhDNw5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WF</th>\n",
       "      <th>Time</th>\n",
       "      <th>NWP1_00h_D-2_U</th>\n",
       "      <th>NWP1_00h_D-2_V</th>\n",
       "      <th>NWP1_00h_D-2_T</th>\n",
       "      <th>NWP1_06h_D-2_U</th>\n",
       "      <th>NWP1_06h_D-2_V</th>\n",
       "      <th>NWP1_06h_D-2_T</th>\n",
       "      <th>NWP1_12h_D-2_U</th>\n",
       "      <th>NWP1_12h_D-2_V</th>\n",
       "      <th>...</th>\n",
       "      <th>NWP4_12h_D-1_U</th>\n",
       "      <th>NWP4_12h_D-1_V</th>\n",
       "      <th>NWP4_12h_D-1_CLCT</th>\n",
       "      <th>NWP4_00h_D_U</th>\n",
       "      <th>NWP4_00h_D_V</th>\n",
       "      <th>NWP4_00h_D_CLCT</th>\n",
       "      <th>NWP4_12h_D_U</th>\n",
       "      <th>NWP4_12h_D_V</th>\n",
       "      <th>NWP4_12h_D_CLCT</th>\n",
       "      <th>Production</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-03 03:00:00</td>\n",
       "      <td>4.4229</td>\n",
       "      <td>-16.527</td>\n",
       "      <td>288.68</td>\n",
       "      <td>6.3428</td>\n",
       "      <td>-16.317</td>\n",
       "      <td>288.57</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>-15.495</td>\n",
       "      <td>...</td>\n",
       "      <td>4.295196</td>\n",
       "      <td>-7.850803</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>4.450514</td>\n",
       "      <td>-6.487014</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-03 04:00:00</td>\n",
       "      <td>5.0740</td>\n",
       "      <td>-16.985</td>\n",
       "      <td>288.40</td>\n",
       "      <td>6.7567</td>\n",
       "      <td>-16.895</td>\n",
       "      <td>288.23</td>\n",
       "      <td>2.7426</td>\n",
       "      <td>-16.285</td>\n",
       "      <td>...</td>\n",
       "      <td>4.295196</td>\n",
       "      <td>-8.693379</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>4.369153</td>\n",
       "      <td>-7.600720</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-03 05:00:00</td>\n",
       "      <td>5.8899</td>\n",
       "      <td>-17.983</td>\n",
       "      <td>288.21</td>\n",
       "      <td>6.8435</td>\n",
       "      <td>-17.223</td>\n",
       "      <td>287.99</td>\n",
       "      <td>3.0243</td>\n",
       "      <td>-17.227</td>\n",
       "      <td>...</td>\n",
       "      <td>4.292873</td>\n",
       "      <td>-8.995411</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>3.888633</td>\n",
       "      <td>-8.621414</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-03 06:00:00</td>\n",
       "      <td>6.1131</td>\n",
       "      <td>-18.897</td>\n",
       "      <td>287.97</td>\n",
       "      <td>7.1638</td>\n",
       "      <td>-17.580</td>\n",
       "      <td>287.95</td>\n",
       "      <td>3.4257</td>\n",
       "      <td>-17.655</td>\n",
       "      <td>...</td>\n",
       "      <td>4.220859</td>\n",
       "      <td>-9.930776</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>3.100145</td>\n",
       "      <td>-9.882844</td>\n",
       "      <td>0.035084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    WF                Time  NWP1_00h_D-2_U  NWP1_00h_D-2_V  NWP1_00h_D-2_T  \\\n",
       "ID                                                                           \n",
       "51   1 2018-05-03 03:00:00          4.4229         -16.527          288.68   \n",
       "52   1 2018-05-03 04:00:00          5.0740         -16.985          288.40   \n",
       "53   1 2018-05-03 05:00:00          5.8899         -17.983          288.21   \n",
       "54   1 2018-05-03 06:00:00          6.1131         -18.897          287.97   \n",
       "\n",
       "    NWP1_06h_D-2_U  NWP1_06h_D-2_V  NWP1_06h_D-2_T  NWP1_12h_D-2_U  \\\n",
       "ID                                                                   \n",
       "51          6.3428         -16.317          288.57          2.8521   \n",
       "52          6.7567         -16.895          288.23          2.7426   \n",
       "53          6.8435         -17.223          287.99          3.0243   \n",
       "54          7.1638         -17.580          287.95          3.4257   \n",
       "\n",
       "    NWP1_12h_D-2_V  ...  NWP4_12h_D-1_U  NWP4_12h_D-1_V  NWP4_12h_D-1_CLCT  \\\n",
       "ID                  ...                                                      \n",
       "51         -15.495  ...        4.295196       -7.850803          -0.000016   \n",
       "52         -16.285  ...        4.295196       -8.693379          -0.000016   \n",
       "53         -17.227  ...        4.292873       -8.995411          -0.000016   \n",
       "54         -17.655  ...        4.220859       -9.930776          -0.000016   \n",
       "\n",
       "    NWP4_00h_D_U  NWP4_00h_D_V  NWP4_00h_D_CLCT  NWP4_12h_D_U  NWP4_12h_D_V  \\\n",
       "ID                                                                            \n",
       "51      4.450514     -6.487014        -0.000013           NaN           NaN   \n",
       "52      4.369153     -7.600720        -0.000013           NaN           NaN   \n",
       "53      3.888633     -8.621414        -0.000013           NaN           NaN   \n",
       "54      3.100145     -9.882844         0.035084           NaN           NaN   \n",
       "\n",
       "    NWP4_12h_D_CLCT  Production  \n",
       "ID                               \n",
       "51              NaN        5.39  \n",
       "52              NaN        5.11  \n",
       "53              NaN        5.03  \n",
       "54              NaN        7.18  \n",
       "\n",
       "[4 rows x 105 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = core.load_data()\n",
    "df.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A given **training example** is thus composed of :\n",
    " - a **target time** (*Time* column) and a **wind farm ID** (*WF* column).\n",
    " - several **weather forecasts** for that (*Time*,*WF*) couple, in the form of *runs* (*NWP\\<i>_\\<HourOfTheRun>_\\<DayOfTheRun>_\\<Variable>* columns). Each run provides an estimation of a particular weather *Variable*, produced at a given time before the target *Time* (*HourOfTheRun*, *DayOfTheRun*), and coming from a given NWP models (*i*). For instance, the run *NWP1_00h_D-2_U* is estimating the weather variable *U* for a given target *Time* using the first NWP model, and is produced at midnight two days before the target *Time*. \n",
    " - the **observed power production** (*Production* column) for that (*Time*,*WF*) couple.\n",
    "\n",
    "The runs are coming from 4 different NWP models ($i\\in[1,4]$), and are forecasting 4 weather variables at various time:\n",
    " \n",
    "NWP Variable | Prediction description | NWP 1 (hourly) | NWP 2 (every 3 hours) | NWP 3 (every 3 hours) | NWP 4 (hourly)\n",
    "------ | ----- | ----- | ----- | ----- | -----\n",
    "Wind speed U,V (m/s) | 10min average [H-10min,H] | x (@100m) | x (@100m) | x (@100m) | x (@10m)\n",
    "Temperature of air T (m/s) | 1hour average [H-1,H] | x |  | x |\n",
    "Total cloud cover CLCT (%) | instant value at H | | | | x\n",
    "\n",
    "Further details about these forecasts can be found on the challenge webpage (link above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of training examples per wind farm\n",
    "df.assign(training = df.Production.isna(), test = ~df.Production.isna())[['WF','training','test']].groupby('WF').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reshaping\n",
    "\n",
    "In order to train a model, we first need to design and shape the training examples that will feed it. In this problem, the learning features are the different weather forecasts, and the target output is the observed power production. \n",
    "\n",
    "Here are some characteristics of the NWP forecasts:\n",
    " - some NWP models are not forecasting their weather variables hourly.\n",
    " - a NWP model is forecasting a given weather variable several times before the target time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some NWP models are not forecasting a given weather variable hourly\n",
    "df.loc[51:54,['WF','Time']+[f'NWP{i}_00h_D-2_U' for i in range(1,5)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given target time, a NWP model is forecasting a weather variable several times (at different delays before the target time).\n",
    "df.loc[51:54,['WF','Time']+[col for col in df.columns if col.startswith('NWP4') and col.endswith('_U')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to deal with the induced missing values is to **compute a new matrix containing the best weather forecast for each (*WF*, *NWP*, *Variable*) triplet**. In other words, each of these triplet is reduced to only one value. For instance, I will create a new single feature called *NWP4_U* giving the best forecast for the wind component *U* forecasted by the fourth NWP model, using the different forecasts *NWP4_XXh-XXX_U*.\n",
    "\n",
    "To do that, I am using a **weighted mean of the forecasts with a memory coefficient <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha\"/>** as hyperparameter. This allows to make recent forecasts more predominant in the calculation than the older ones. We have then : \n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V_{best}=\\dfrac{\\sum_{k=1}^{n}\\alpha^{\\Delta H_k}\\,V_k}{\\sum_{k=1}^{n}\\alpha^{\\Delta H_k}}\"/>\n",
    "\n",
    "where <img src=\"https://render.githubusercontent.com/render/math?math=V_k\"/> is the k-th prediction made for a given triplet, which has been produced <img src=\"https://render.githubusercontent.com/render/math?math=\\Delta H_k\"/> hours before the target time. \n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=\\alpha\"/> is a memory coefficient lying in <img src=\"https://render.githubusercontent.com/render/math?math=]0,1]\"/>, which make the value weight <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha^{\\Delta H_k}\"/> decaying as the delay <img src=\"https://render.githubusercontent.com/render/math?math=\\Delta H_k\"/> increases. If <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha=1\"/>, all predictions have the same weight (classic mean). Instead, if <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha\"/> tends towards 0, we are just keeping the more recent forecast. \n",
    "\n",
    "Let's start with <img src=\"https://render.githubusercontent.com/render/math?math=\\alpha=0.9\"/>, meaning that the (H-1) forecast, if existing, has weight 0.9, then the (H-2) forecast has weight <img src=\"https://render.githubusercontent.com/render/math?math=(0.9)^2=0.81\"/>, the (H-12) forecast has weight <img src=\"https://render.githubusercontent.com/render/math?math=(0.9)^12=0.28\"/>, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute best weather forecasts\n",
    "FORECAST_MEMORY = 0.9\n",
    "df = core.calculate_best_forecasts(df, FORECAST_MEMORY)\n",
    "df.loc[51:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that we have drastically reduced the number of features, by making new ones more meaningful for training the future ML/DL model. But we are still facing missing values, due to the fact that there are no forecasts at all for these weather variable at these times. A fairly straightforward approach here is thus to **linearly interpolate these missing values using the forecasts made for previous and future times**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate remaining missing values\n",
    "df = core.interpolate_nans(df)\n",
    "df.loc[51:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqDy19JNDNw6"
   },
   "outputs": [],
   "source": [
    "# Interpolate remaining missing values\n",
    "df = core.augment_data(df)\n",
    "df = core.normalize_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piGHgpwSDNw6"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYtVmg6WdGjc"
   },
   "outputs": [],
   "source": [
    "# Model parameters\r\n",
    "WINDOW_SIZE = 72  # In hours\r\n",
    "BATCH_SIZE = 2000\r\n",
    "EPOCHS = 10\r\n",
    "UNITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF1wTcwTDNw6"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(t_train, x_train, y_train, t_valid, x_valid, y_valid):\n",
    "\n",
    "    mlflow.log_params({'window_size':WINDOW_SIZE, 'units':UNITS, 'layer_type':'GRU'})\n",
    "    \n",
    "    # Make learning datasets\n",
    "    dataset_train = core.get_windowed_dataset(x_train, y_train, WINDOW_SIZE, BATCH_SIZE, shuffle=True)\n",
    "    dataset_valid = core.get_windowed_dataset(x_valid, y_valid, WINDOW_SIZE, BATCH_SIZE, shuffle=False) if x_valid is not None else None\n",
    "        \n",
    "    # Define model\n",
    "    model = tf.keras.Sequential([\n",
    "                tf.keras.layers.InputLayer(input_shape=next(iter(dataset_train))[0].shape[1:]),\n",
    "                tf.keras.layers.GRU(UNITS, return_sequences=True),\n",
    "                tf.keras.layers.Dropout(0.6),\n",
    "                tf.keras.layers.GRU(UNITS, return_sequences=True),\n",
    "                tf.keras.layers.Dropout(0.6),\n",
    "                tf.keras.layers.GRU(UNITS),\n",
    "                tf.keras.layers.Dense(1, activation='relu')\n",
    "            ])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=tf.keras.optimizers.Adam())\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(dataset_train, \n",
    "                        validation_data=dataset_valid,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=1, \n",
    "                        callbacks=[])#tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001,patience=30)])\n",
    "    core.plot_learning_curves(history)\n",
    "    \n",
    "    # Check predictions\n",
    "    y_train_predict = core.predict(model, dataset_train, t_train)\n",
    "    core.plot_predictions(t_train, y_train, y_train_predict, 'train')\n",
    "    if dataset_valid is not None :\n",
    "        y_valid_predict = core.predict(model, dataset_valid, t_valid)\n",
    "        core.plot_predictions(t_valid, y_valid, y_valid_predict, 'valid')\n",
    "            \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l1P4URCZrUT"
   },
   "source": [
    "# Holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN6vRa-rdZcl"
   },
   "outputs": [],
   "source": [
    "HOLDOUT_VAL_SPLIT = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08_Y_Us5DNw6"
   },
   "outputs": [],
   "source": [
    "# TRAIN ONLY ONE WIND FARM\n",
    "# ================================\n",
    "def train_holdout_validation(wf_num, nested_run=False) :\n",
    "\n",
    "    with mlflow.start_run(nested=nested_run):    \n",
    "        mlflow.log_params({'wf':wf_num, 'split':HOLDOUT_VAL_SPLIT})\n",
    "\n",
    "        # Extract wf data\n",
    "        df_wf = core.extract_wf_data(df, wf_num)\n",
    "        \n",
    "        # Train\n",
    "        t_train, x_train, y_train, t_valid, x_valid, y_valid = core.split_holdout_validation(df_wf, HOLDOUT_VAL_SPLIT, WINDOW_SIZE)\n",
    "        model, history = train_model(t_train, x_train, y_train, t_valid, x_valid, y_valid)\n",
    "            \n",
    "    return model, history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50QVcEqgzlzc"
   },
   "outputs": [],
   "source": [
    "# Train one wind farm\n",
    "mlflow.set_experiment('holdout_validation')\n",
    "model, history = train_holdout_validation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY_7A48AOMCF"
   },
   "outputs": [],
   "source": [
    "# Train all wind farms\n",
    "mlflow.set_experiment('holdout_validation')\n",
    "with mlflow.start_run() :\n",
    "    for wf_num in df.WF.unique(): \n",
    "        train_holdout_validation(wf_num, nested_run = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4xRxSDtv0e3"
   },
   "source": [
    "# Forward chaining validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoLW8rJ7d-Fg"
   },
   "outputs": [],
   "source": [
    "# Forward chaining parameters\r\n",
    "FC_VAL_NB = 4\r\n",
    "FC_VAL_SIZE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdriGb82v0e3"
   },
   "outputs": [],
   "source": [
    "def train_forward_chaining_validation(wf_num):\n",
    "\n",
    "    # Extract wf data\n",
    "    df_wf = core.extract_wf_data(df, wf_num)\n",
    "    \n",
    "    # Train models\n",
    "    metrics = []\n",
    "    datas = core.split_forward_chaining_validation(df_wf, FC_VAL_SIZE, FC_VAL_NB, WINDOW_SIZE)\n",
    "    for (t_train, x_train, y_train, t_valid, x_valid, y_valid) in datas :\n",
    "        with mlflow.start_run(nested=True) :\n",
    "            model, history = train_model(t_train, x_train, y_train, t_valid, x_valid, y_valid)\n",
    "            metrics.append(history.history)\n",
    "    \n",
    "    # Calculate mean and std errors\n",
    "    metrics = core.get_mean_std_metrics(metrics)\n",
    "    mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKdYEe9Ox1k_"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment('forward_chaining_validation')\n",
    "for wf_num in df.WF.unique():\n",
    "    with mlflow.start_run():    \n",
    "        mlflow.log_params({'wf':wf_num,'valid_size':FC_VAL_SIZE,'nb_valid':FC_VAL_NB, 'nlayers':3, \n",
    "                        'layer_type':'GRU','units':UNITS,'epochs':EPOCHS, 'parent':True})\n",
    "        train_forward_chaining_validation(wf_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC0ea6kQgr1J"
   },
   "outputs": [],
   "source": [
    "wf_num = 4\r\n",
    "mlflow.set_experiment('forward_chaining_validation')\r\n",
    "for UNITS in [32,64] :\r\n",
    "    with mlflow.start_run() :\r\n",
    "        mlflow.log_params({'wf':wf_num,'valid_size':FC_VAL_SIZE,'nb_valid':FC_VAL_NB, 'nlayers':1, \r\n",
    "                           'layer_type':'GRU', 'units':UNITS, 'epochs':EPOCHS, 'parent':True })\r\n",
    "        train_forward_chaining_validation(wf_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK3nZypssKI2"
   },
   "source": [
    "# Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6SuOmlPDNw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN ALL WIND FARMS AND PREDICT\n",
    "# ================================\n",
    "def train_full(wf_num, nested_run=False) :\n",
    "    with mlflow.start_run(nested=nested_run): \n",
    "        mlflow.log_param('wf',wf_num)\n",
    "\n",
    "        # Extract data\n",
    "        df_wf = core.extract_wf_data(df, wf_num)        \n",
    "        \n",
    "        # Train model\n",
    "        t_train, x_train, y_train = core.get_train_dataset(df_wf, WINDOW_SIZE)\n",
    "        model, history = train_model(t_train, x_train, y_train, None, None, None)\n",
    "\n",
    "        # Predict on test data\n",
    "        t_test, x_test = core.get_test_dataset(df_wf, WINDOW_SIZE)\n",
    "        dataset_test = core.get_windowed_dataset(x_test, None, WINDOW_SIZE, BATCH_SIZE, shuffle=False)\n",
    "        y_test_predict = core.predict(model, dataset_test, t_test)    \n",
    "        core.plot_predictions(t_test, None, y_test_predict, 'test')\n",
    "\n",
    "    return y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67BQ2u3Azlzh"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment('Full training')\n",
    "with mlflow.start_run():\n",
    "    predictions = [train_full(wf_num, nested_run=True) for wf_num in df.WF.unique()]\n",
    "    core.save_predictions(predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_testsetval.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wind-power-forecasting-challenge",
   "language": "python",
   "name": "wind-power-forecasting-challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
